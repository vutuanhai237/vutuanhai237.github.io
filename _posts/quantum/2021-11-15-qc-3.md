---
layout: post
title: "QML #3: Natural gradient descent"
categories: quantum
link: https://vutuanhai237.github.io/quantum/2021/11/15/qc-3.html
---

### **Giới thiệu**

Với standard gradient descent, chúng ta đã biết được quy trình để cực tiểu hóa giá trị của một hàm $f$ với tham số $\theta$ như sau:

1. Chọn giá trị khởi tạo cho $\theta$, $\theta=\theta_0$, có thể ngẫu nhiên, hoặc [Glorot](http://proceedings.mlr.press/v9/glorot10a.html), hoặc [He](https://ieeexplore.ieee.org/document/7410480).

2. Tính đạo hàm riêng:

$$
\begin{aligned}
\nabla f_{\theta_i} = \frac{\partial f(\theta)}{\theta_i}
\end{aligned}
$$

3. Cập nhật giá trị mới cho $\theta$ bằng một số Optimizer, tiêu biểu là [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) hoặc [Adam](https://arxiv.org/abs/1412.6980). Với SGD:

$$
\begin{aligned}
\theta^{t+1} = \theta^{t} - \alpha\nabla f_\theta
\end{aligned}
$$

Sau đó, chúng ta quay lại bước 2 và tính trong giới hạn $n$ lượt nào đó hoặc cho đến khi giá trị hàm Loss được tối ưu (có thể là cực tiểu hóa hoặc cực đại hóa, tùy theo cách chúng ta định nghĩa).

Tuy nhiên, khi đối mặt với những bài toán khó, hàm Loss mất rất nhiều thời gian để tối ưu hoặc có thể không bao giờ tối ưu được, thì cho dù chúng ta sử dụng bất kì Optimizer cổ điển nào thì cũng không thể giải quyết được. Hiện tượng này được gọi là Barren plateaus.

<center><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR469pVY19tZBOnlfGeQ6Zz2YeI9-hAcrWOg55kUoPWWN4mCfqz45kLTLU7niiziqNnr3c&usqp=CAU"></center>

Để dễ tưởng tượng thì không gian tham số của chúng ta giống nhau một cao nguyên vô tận, có những đỉnh đồi nhưng đa số lại hoàn toàn phẳng. Kết quả là chúng ta không biết chỗ nào là hướng cần đi, và cuối cùng mắc kẹt tại chỗ cũ. Một vấn đề khác cũng có tính chất khá giống đó là rơi vào local minimum và rất khó để thoát khỏi và tìm global minumum.

Để giải quyết vấn đề này, chúng ta sẽ sử dụng một số kĩ thuật chẳng hạn như adapt learning rate hoặc adapt batch size, ... Tuy nhiên có một phương pháp có vẻ triển vọng hơn.

#### 1. Natural gradient descent (NGD)

SGD thông thường sẽ cập nhật $\theta$ bằng cách sử dụng Euclide metric, vốn không phù hợp trong không gian tham số. Trong khi đó, NGD sẽ tận dụng lợi thế của không gian tham số này.

<center><img src="https://pennylane.ai/qml/_images/qng7.png"></center>






Tài liệu tham khảo:

[1] https://pennylane.ai/qml/glossary/parameter_shift.html

[2] Schuld, Maria, et al. "Evaluating analytic gradients on quantum hardware." Physical Review A 99.3 (2019): 032331.

[3] Broughton, Michael, et al. "Tensorflow quantum: A software framework for quantum machine learning." arXiv preprint arXiv:2003.02989 (2020).

[4] Mitarai, Kosuke, et al. "Quantum circuit learning." Physical Review A 98.3 (2018): 032309.
